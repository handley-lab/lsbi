{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Handling Complexity with Mixture Models\n",
    "\n",
    "What happens when your data doesn't come from a single, simple process? What if it could be from one of two (or more) different models? A single `LinearModel` will fail to capture this complexity.\n",
    "\n",
    "This is where `lsbi.MixtureModel` comes in. It allows you to model data as a mixture of several linear components, and infer which component is most likely to have generated the data.\n",
    "\n",
    "We will cover:\n",
    "1. The problem: generating bimodal data that can't be fit by a single line\n",
    "2. Failure of a single `LinearModel` \n",
    "3. The solution: `MixtureModel` with multiple components\n",
    "4. Interpreting mixture model results\n",
    "5. Advanced mixture model techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Problem: A Bimodal Dataset\n",
    "\n",
    "Let's generate data from two different lines and mix them together to create a challenging dataset that no single linear model can fit well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bimodal data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lsbi import LinearModel, MixtureModel\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "# Model 1: Positive slope line\n",
    "theta_1 = np.array([2.0, 1.0])   # slope, intercept\n",
    "x_1 = np.linspace(-2, 2, 50)\n",
    "y_1 = x_1 * theta_1[0] + theta_1[1] + np.random.normal(0, 0.4, size=x_1.shape)\n",
    "\n",
    "# Model 2: Negative slope line  \n",
    "theta_2 = np.array([-2.0, -1.0]) # slope, intercept\n",
    "x_2 = np.linspace(-2, 2, 50)\n",
    "y_2 = x_2 * theta_2[0] + theta_2[1] + np.random.normal(0, 0.4, size=x_2.shape)\n",
    "\n",
    "# Combine into a single dataset - this creates an \"X\" pattern\n",
    "combined_x = np.concatenate([x_1, x_2])\n",
    "combined_y = np.concatenate([y_1, y_2])\n",
    "\n",
    "print(f\"Dataset size: {len(combined_x)} points\")\n",
    "print(f\"True parameters - Component 1: {theta_1}\")\n",
    "print(f\"True parameters - Component 2: {theta_2}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_1, y_1, alpha=0.6, label='Component 1 Data', color='blue')\n",
    "plt.scatter(x_2, y_2, alpha=0.6, label='Component 2 Data', color='red')\n",
    "plt.plot(x_1, x_1 * theta_1[0] + theta_1[1], 'b-', lw=2, label='True Line 1')\n",
    "plt.plot(x_2, x_2 * theta_2[0] + theta_2[1], 'r-', lw=2, label='True Line 2')\n",
    "plt.title(\"A Bimodal Dataset: Two Linear Processes\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Failure of a Single `LinearModel`\n",
    "\n",
    "Let's see what happens when we try to fit this \"X\"-shaped data with a single straight line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempting to fit with a single model\n",
    "M_single = np.vstack([combined_x, np.ones_like(combined_x)]).T\n",
    "model_single = LinearModel(M=M_single, n=2, d=len(combined_x))  # Using default priors\n",
    "\n",
    "posterior_single = model_single.posterior(combined_y)\n",
    "\n",
    "print(\"Single model fit:\")\n",
    "print(f\"Estimated parameters: {np.round(posterior_single.mean, 2)}\")\n",
    "print(f\"Parameter uncertainty (std): {np.round(np.sqrt(np.diag(posterior_single.cov)), 2)}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(combined_x, combined_y, alpha=0.6, label='Combined Data', color='gray')\n",
    "plt.plot(combined_x, M_single @ posterior_single.mean, 'orange', lw=3, label='Single Model Fit')\n",
    "\n",
    "# Show the true generating lines for reference\n",
    "plt.plot(x_1, x_1 * theta_1[0] + theta_1[1], 'b--', lw=2, alpha=0.7, label='True Line 1')\n",
    "plt.plot(x_2, x_2 * theta_2[0] + theta_2[1], 'r--', lw=2, alpha=0.7, label='True Line 2')\n",
    "\n",
    "plt.title(\"Failure of a Single Linear Model\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Calculate the evidence (log probability of the data)\n",
    "evidence_single = model_single.evidence()\n",
    "log_evidence_single = evidence_single.logpdf(combined_y)\n",
    "print(f\"\\nSingle model log-evidence: {log_evidence_single:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the single model produces a poor compromise fit that doesn't represent either underlying process well. The orange line tries to average between the two trends, resulting in a nearly flat line that fits neither component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Solution: `MixtureModel`\n",
    "\n",
    "We will now model this as a mixture of two `LinearModel`s. This requires \"stacking\" the parameters for each component into arrays with a leading dimension equal to the number of components (in our case, 2).\n",
    "\n",
    "**Key Concept**: A `MixtureModel` with `k` components needs:\n",
    "- `mu`: shape `(k, n)` \n",
    "- `Sigma`: shape `(k, n, n)`\n",
    "- `M`: shape `(k, d, n)`\n",
    "- `C`: shape `(k, d, d)`\n",
    "- `logw`: shape `(k,)` - log prior weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the MixtureModel\n",
    "n_components = 2\n",
    "n_params = 2\n",
    "n_data = len(combined_x)\n",
    "\n",
    "print(f\"Setting up MixtureModel with {n_components} components\")\n",
    "print(f\"Problem size: {n_params} parameters, {n_data} data points\")\n",
    "\n",
    "# Priors for the two components, stacked along a new axis\n",
    "# We'll use broad priors centered around zero for both components\n",
    "mu_mix = np.zeros((n_components, n_params))\n",
    "print(f\"mu_mix shape: {mu_mix.shape}\")\n",
    "\n",
    "# Same prior covariance for both components\n",
    "Sigma_mix = np.tile(np.eye(n_params) * 10, (n_components, 1, 1))\n",
    "print(f\"Sigma_mix shape: {Sigma_mix.shape}\")\n",
    "\n",
    "# The M matrix is the same for both components in this case\n",
    "M_mix = np.tile(M_single, (n_components, 1, 1))\n",
    "print(f\"M_mix shape: {M_mix.shape}\")\n",
    "\n",
    "# The data covariance is also the same for both components\n",
    "C_mix = np.tile(np.eye(n_data) * 0.4**2, (n_components, 1, 1))\n",
    "print(f\"C_mix shape: {C_mix.shape}\")\n",
    "\n",
    "# Prior weights: we assume each component is equally likely a priori\n",
    "logw = np.log([0.5, 0.5])\n",
    "print(f\"logw shape: {logw.shape}\")\n",
    "print(f\"Prior probabilities: {np.exp(logw)}\")\n",
    "\n",
    "# Instantiate the MixtureModel\n",
    "mixture_model = MixtureModel(M=M_mix, mu=mu_mix, Sigma=Sigma_mix, C=C_mix, logw=logw)\n",
    "print(f\"\\n✓ MixtureModel created with {mixture_model.k} components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common Pitfall Alert**: Getting the shapes right is crucial. The most common errors are:\n",
    "- Forgetting the leading component dimension\n",
    "- Using the wrong axis for stacking\n",
    "- Inconsistent shapes between different arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference and Analysis with `MixtureModel`\n",
    "\n",
    "Now we perform inference and inspect the results. The posterior from a `MixtureModel` contains the individual posteriors for each component, along with the updated posterior weights `logw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference and Visualization\n",
    "print(\"Computing mixture model posterior...\")\n",
    "mixture_posterior = mixture_model.posterior(combined_y)\n",
    "\n",
    "# The posterior mean is now an array of shape (n_components, n_params)\n",
    "posterior_means = mixture_posterior.mean\n",
    "posterior_covs = mixture_posterior.cov\n",
    "\n",
    "print(\"\\n=== Mixture Model Results ===\")\n",
    "print(f\"Posterior means for each component:\")\n",
    "for i in range(n_components):\n",
    "    print(f\"  Component {i+1}: {np.round(posterior_means[i], 2)}\")\n",
    "    \n",
    "print(f\"\\nTrue parameters were:\")\n",
    "print(f\"  Component 1: {theta_1}\")\n",
    "print(f\"  Component 2: {theta_2}\")\n",
    "\n",
    "# The posterior weights tell us the probability that each component generated the data\n",
    "posterior_weights = np.exp(mixture_posterior.logw)\n",
    "print(f\"\\nComponent weights:\")\n",
    "print(f\"  Prior weights:     {np.round(np.exp(logw), 3)}\")\n",
    "print(f\"  Posterior weights: {np.round(posterior_weights, 3)}\")\n",
    "\n",
    "# Calculate mixture model evidence\n",
    "evidence_mixture = mixture_model.evidence()\n",
    "log_evidence_mixture = evidence_mixture.logpdf(combined_y)\n",
    "print(f\"\\nModel comparison:\")\n",
    "print(f\"  Single model log-evidence:  {log_evidence_single:.2f}\")\n",
    "print(f\"  Mixture model log-evidence: {log_evidence_mixture:.2f}\")\n",
    "print(f\"  Evidence ratio (mixture/single): {np.exp(log_evidence_mixture - log_evidence_single):.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the mixture model fits\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(combined_x, combined_y, alpha=0.6, label='Combined Data', color='gray', s=30)\n",
    "\n",
    "# Plot the mixture model component fits\n",
    "colors = ['blue', 'red']\n",
    "for i in range(n_components):\n",
    "    line_y = M_single @ posterior_means[i]\n",
    "    plt.plot(combined_x, line_y, color=colors[i], lw=3, \n",
    "             label=f'Component {i+1} Fit (w={posterior_weights[i]:.2f})')\n",
    "    \n",
    "    # Plot uncertainty bands using posterior covariance\n",
    "    # Sample from posterior to show uncertainty\n",
    "    component_posterior = mixture_posterior[i]\n",
    "    samples = component_posterior.rvs(size=100)\n",
    "    for j in range(min(20, len(samples))):\n",
    "        sample_line = M_single @ samples[j]\n",
    "        plt.plot(combined_x, sample_line, color=colors[i], alpha=0.1, lw=1)\n",
    "\n",
    "# Compare with the failed single model\n",
    "plt.plot(combined_x, M_single @ posterior_single.mean, \n",
    "         'orange', lw=2, linestyle='--', alpha=0.8, label='Single Model (Failed)')\n",
    "\n",
    "# Show true generating lines\n",
    "plt.plot(x_1, x_1 * theta_1[0] + theta_1[1], 'b:', lw=2, alpha=0.8, label='True Line 1')\n",
    "plt.plot(x_2, x_2 * theta_2[0] + theta_2[1], 'r:', lw=2, alpha=0.8, label='True Line 2')\n",
    "\n",
    "plt.title(\"Successful Fit with MixtureModel\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Success! The MixtureModel correctly identified both underlying trends.\")\n",
    "print(f\"✓ Component weights are balanced ({posterior_weights[0]:.2f}, {posterior_weights[1]:.2f})\")\n",
    "print(f\"  as expected since we have equal amounts of data from each component.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding Mixture Model Components\n",
    "\n",
    "Let's dive deeper into what the mixture model learned by examining each component individually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze individual components\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Component parameter distributions\n",
    "theta_range = np.linspace(-4, 4, 200)\n",
    "colors = ['blue', 'red']\n",
    "\n",
    "for i in range(n_components):\n",
    "    component_post = mixture_posterior[i]\n",
    "    \n",
    "    # Slope distribution\n",
    "    slope_dist = component_post.marginalise([1])  # Remove intercept, keep slope\n",
    "    axes[0, i].plot(theta_range, slope_dist.pdf(theta_range[:, None]).flatten(), \n",
    "                   color=colors[i], lw=2, label=f'Component {i+1} Slope')\n",
    "    axes[0, i].axvline([theta_1, theta_2][i][0], color='black', linestyle='--', \n",
    "                      label=f'True Slope ({[theta_1, theta_2][i][0]})')\n",
    "    axes[0, i].set_title(f'Component {i+1}: Slope Distribution')\n",
    "    axes[0, i].set_xlabel('Slope')\n",
    "    axes[0, i].set_ylabel('Density')\n",
    "    axes[0, i].legend()\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Intercept distribution\n",
    "    intercept_dist = component_post.marginalise([0])  # Remove slope, keep intercept\n",
    "    axes[1, i].plot(theta_range, intercept_dist.pdf(theta_range[:, None]).flatten(), \n",
    "                   color=colors[i], lw=2, label=f'Component {i+1} Intercept')\n",
    "    axes[1, i].axvline([theta_1, theta_2][i][1], color='black', linestyle='--', \n",
    "                      label=f'True Intercept ({[theta_1, theta_2][i][1]})')\n",
    "    axes[1, i].set_title(f'Component {i+1}: Intercept Distribution')\n",
    "    axes[1, i].set_xlabel('Intercept')\n",
    "    axes[1, i].set_ylabel('Density')\n",
    "    axes[1, i].legend()\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print quantitative comparison\n",
    "print(\"=== Quantitative Assessment ===\")\n",
    "for i in range(n_components):\n",
    "    true_params = [theta_1, theta_2][i]\n",
    "    estimated_params = posterior_means[i]\n",
    "    param_std = np.sqrt(np.diag(posterior_covs[i]))\n",
    "    \n",
    "    print(f\"\\nComponent {i+1}:\")\n",
    "    print(f\"  True parameters:      [{true_params[0]:5.2f}, {true_params[1]:5.2f}]\")\n",
    "    print(f\"  Estimated parameters: [{estimated_params[0]:5.2f}, {estimated_params[1]:5.2f}]\")\n",
    "    print(f\"  Parameter std errors: [{param_std[0]:5.2f}, {param_std[1]:5.2f}]\")\n",
    "    \n",
    "    # Calculate how many standard deviations away the estimate is\n",
    "    z_scores = np.abs(estimated_params - true_params) / param_std\n",
    "    print(f\"  |Z-scores|:           [{z_scores[0]:5.2f}, {z_scores[1]:5.2f}]\")\n",
    "    if all(z_scores < 2):\n",
    "        print(f\"  ✓ Excellent fit (< 2σ error)\")\n",
    "    elif all(z_scores < 3):\n",
    "        print(f\"  ✓ Good fit (< 3σ error)\")\n",
    "    else:\n",
    "        print(f\"  ⚠ Poor fit (> 3σ error)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Topics: Monte Carlo Estimates and Information Gain\n",
    "\n",
    "Unlike `LinearModel`, mixture models require Monte Carlo sampling for some calculations like the KL divergence. Let's explore this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo estimates for mixture models\n",
    "print(\"=== Information Gain Analysis ===\")\n",
    "\n",
    "# For a single LinearModel, KL divergence is computed analytically\n",
    "try:\n",
    "    kl_single = model_single.dkl(combined_y)\n",
    "    print(f\"Single model KL divergence: {kl_single:.3f} nats (analytical)\")\nexcept Exception as e:\n",
    "    print(f\"Single model KL error: {e}\")\n",
    "\n",
    "# For MixtureModel, we need Monte Carlo sampling\n",
    "try:\n",
    "    # This will fail - mixture models require n > 0\n",
    "    kl_mixture = mixture_model.dkl(combined_y)\n",
    "    print(f\"Mixture model KL divergence: {kl_mixture:.3f} nats\")\nexcept Exception as e:\n",
    "    print(f\"Mixture model KL error (expected): {str(e)}\")\n",
    "\n",
    "# Use Monte Carlo estimation\n",
    "n_samples = 5000\n",
    "print(f\"\\nUsing Monte Carlo with {n_samples} samples...\")\n",
    "kl_mixture_mc = mixture_model.dkl(combined_y, n=n_samples)\n",
    "print(f\"Mixture model KL divergence: {kl_mixture_mc:.3f} nats (Monte Carlo)\")\n",
    "\n",
    "# Compare information gains\n",
    "print(f\"\\nInformation Gain Comparison:\")\n",
    "print(f\"  Single model:  {kl_single:.3f} nats ({kl_single/np.log(2):.2f} bits)\")\n",
    "print(f\"  Mixture model: {kl_mixture_mc:.3f} nats ({kl_mixture_mc/np.log(2):.2f} bits)\")\n",
    "print(f\"  Additional information from mixture: {(kl_mixture_mc-kl_single)/np.log(2):.2f} bits\")\n",
    "\n",
    "# Show convergence of Monte Carlo estimate\n",
    "sample_sizes = [100, 500, 1000, 2000, 5000, 10000]\n",
    "kl_estimates = []\n",
    "\n",
    "print(f\"\\nMonte Carlo convergence:\")\n",
    "for n in sample_sizes:\n",
    "    kl_est = mixture_model.dkl(combined_y, n=n)\n",
    "    kl_estimates.append(kl_est)\n",
    "    print(f\"  n={n:5d}: KL = {kl_est:.4f} nats\")\n",
    "\n",
    "# Plot convergence\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.semilogx(sample_sizes, kl_estimates, 'o-', lw=2, markersize=6)\n",
    "plt.axhline(kl_estimates[-1], color='red', linestyle='--', alpha=0.7, \n",
    "           label=f'Converged value: {kl_estimates[-1]:.3f}')\n",
    "plt.xlabel('Number of Monte Carlo Samples')\n",
    "plt.ylabel('KL Divergence (nats)')\n",
    "plt.title('Monte Carlo Convergence for Mixture Model KL Divergence')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Selection: When to Use Mixture Models\n",
    "\n",
    "Let's examine when mixture models are justified using model evidence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection using evidence\n",
    "print(\"=== Model Selection Analysis ===\")\n",
    "\n",
    "# Compare different numbers of components\n",
    "component_counts = [1, 2, 3]\n",
    "log_evidences = []\n",
    "model_names = []\n",
    "\n",
    "for k in component_counts:\n",
    "    if k == 1:\n",
    "        # Single component (our original LinearModel)\n",
    "        evidence = model_single.evidence().logpdf(combined_y)\n",
    "        model_names.append(\"Single LinearModel\")\n",
    "    else:\n",
    "        # Multiple components\n",
    "        # Create a k-component mixture model\n",
    "        mu_k = np.zeros((k, n_params))\n",
    "        Sigma_k = np.tile(np.eye(n_params) * 10, (k, 1, 1))\n",
    "        M_k = np.tile(M_single, (k, 1, 1))\n",
    "        C_k = np.tile(np.eye(n_data) * 0.4**2, (k, 1, 1))\n",
    "        logw_k = np.log(np.ones(k) / k)  # Equal weights\n",
    "        \n",
    "        model_k = MixtureModel(M=M_k, mu=mu_k, Sigma=Sigma_k, C=C_k, logw=logw_k)\n",
    "        evidence = model_k.evidence().logpdf(combined_y)\n",
    "        model_names.append(f\"{k}-Component Mixture\")\n",
    "    \n",
    "    log_evidences.append(evidence)\n",
    "\n",
    "# Display results\n",
    "print(\"Model Evidence Comparison:\")\n",
    "best_idx = np.argmax(log_evidences)\n",
    "for i, (name, log_ev) in enumerate(zip(model_names, log_evidences)):\n",
    "    relative_evidence = np.exp(log_ev - log_evidences[best_idx])\n",
    "    marker = \"👑\" if i == best_idx else \"  \"\n",
    "    print(f\"{marker} {name:20s}: log-evidence = {log_ev:8.2f}, relative = {relative_evidence:.2e}\")\n",
    "\n",
    "print(f\"\\n🏆 Best model: {model_names[best_idx]}\")\n",
    "\n",
    "# Bayes factor interpretation\n",
    "bayes_factor = np.exp(log_evidences[1] - log_evidences[0])  # 2-component vs 1-component\n",
    "print(f\"\\nBayes Factor (2-component vs 1-component): {bayes_factor:.2e}\")\n",
    "if bayes_factor > 100:\n",
    "    strength = \"decisive\"\n",
    "elif bayes_factor > 10:\n",
    "    strength = \"strong\"\n",
    "elif bayes_factor > 3:\n",
    "    strength = \"moderate\"\n",
    "else:\n",
    "    strength = \"weak\"\n",
    "print(f\"Evidence strength: {strength} support for mixture model\")\n",
    "\n",
    "# Visualize model comparison\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(model_names, log_evidences, \n",
    "               color=['orange', 'green', 'purple'], alpha=0.7)\n",
    "bars[best_idx].set_color('gold')\n",
    "bars[best_idx].set_edgecolor('black')\n",
    "bars[best_idx].set_linewidth(2)\n",
    "\n",
    "plt.ylabel('Log Evidence')\n",
    "plt.title('Model Evidence Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we explored `lsbi`'s `MixtureModel` for handling complex, multi-modal data:\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **When to Use Mixture Models**: When your data might come from multiple different linear processes, a single `LinearModel` will produce poor, compromise fits.\n",
    "\n",
    "2. **Shape Requirements**: `MixtureModel` requires stacking parameters along the component dimension:\n",
    "   - `mu`: `(k, n)` for k components and n parameters\n",
    "   - `Sigma`: `(k, n, n)` for parameter covariances\n",
    "   - `M`, `C`: Similarly stacked for each component\n",
    "\n",
    "3. **Interpretation**: The posterior provides:\n",
    "   - Individual parameter estimates for each component\n",
    "   - Updated component weights showing which components best explain the data\n",
    "   - Uncertainty quantification for all estimates\n",
    "\n",
    "4. **Monte Carlo Requirements**: Unlike `LinearModel`, mixture models need Monte Carlo sampling for some calculations (like KL divergence).\n",
    "\n",
    "5. **Model Selection**: Use evidence comparison to determine the optimal number of components. More components aren't always better due to the complexity penalty built into Bayesian model comparison.\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "- **Start Simple**: Try a single `LinearModel` first\n",
    "- **Use Evidence**: Let the data tell you how many components are needed\n",
    "- **Check Convergence**: Use sufficient Monte Carlo samples for stable estimates\n",
    "- **Visualize Results**: Always plot your mixture components to understand what the model learned\n",
    "\n",
    "### When Mixture Models Excel:\n",
    "\n",
    "- **Multi-modal data**: Data that clearly comes from different processes\n",
    "- **Regime changes**: Time series or experiments with distinct phases\n",
    "- **Population studies**: Different subgroups with different linear relationships\n",
    "- **Robust regression**: Handling outliers by treating them as a separate component\n",
    "\n",
    "The mixture model framework in `lsbi` provides a powerful tool for capturing complexity while maintaining the analytical tractability and performance benefits of linear Bayesian inference!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}