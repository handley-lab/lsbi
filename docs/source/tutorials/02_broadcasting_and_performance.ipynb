{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Broadcasting and Performance Optimization\n",
    "\n",
    "In this tutorial, we demonstrate how `lsbi` can be used to efficiently analyze thousands of datasets without explicit `for` loops, and how to gain significant speedups using diagonal covariance optimizations.\n",
    "\n",
    "We will cover:\n",
    "1. The scenario: analyzing many experiments simultaneously\n",
    "2. The slow way: using a `for` loop\n",
    "3. The fast way: `lsbi` broadcasting\n",
    "4. Performance optimization with diagonal covariances\n",
    "5. Best practices for high-dimensional problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Scenario: Many Experiments\n",
    "\n",
    "Imagine you are running the same line-fitting experiment from Tutorial 1, but you run it `5000` times, each time getting a new noisy dataset. Your goal is to find the posterior for each of these `5000` experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for many experiments\n",
    "import numpy as np\n",
    "from lsbi import LinearModel\n",
    "import time\n",
    "\n",
    "# Use the same model setup as Tutorial 1\n",
    "np.random.seed(0)\n",
    "theta_true = np.array([2.5, -1.0])\n",
    "n_params = len(theta_true)\n",
    "x_data = np.linspace(-2, 2, 20)\n",
    "n_data = len(x_data)\n",
    "\n",
    "M = np.vstack([x_data, np.ones_like(x_data)]).T\n",
    "mu = np.zeros(n_params)\n",
    "Sigma = np.eye(n_params) * 10\n",
    "data_noise_std = 0.5\n",
    "C = np.eye(n_data) * data_noise_std**2\n",
    "\n",
    "model = LinearModel(M=M, mu=mu, Sigma=Sigma, C=C)\n",
    "\n",
    "# Generate 5000 noisy datasets\n",
    "num_datasets = 5000\n",
    "y_data_true = M @ theta_true\n",
    "all_y_data_noisy = y_data_true + np.random.normal(0, data_noise_std, size=(num_datasets, n_data))\n",
    "print(\"Shape of all datasets:\", all_y_data_noisy.shape)\n",
    "print(f\"Total data points: {num_datasets * n_data:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Slow Way: Using a `for` loop\n",
    "\n",
    "The naive approach is to loop over each dataset and compute the posterior individually. Let's see how long this takes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The slow for-loop approach\n",
    "print(\"Computing posteriors using a for-loop...\")\n",
    "start_time = time.time()\n",
    "posterior_means_loop = []\n",
    "for i in range(num_datasets):\n",
    "    D = all_y_data_noisy[i]\n",
    "    posterior = model.posterior(D)\n",
    "    posterior_means_loop.append(posterior.mean)\n",
    "    \n",
    "    # Progress indicator\n",
    "    if (i + 1) % 1000 == 0 or i == 0:\n",
    "        print(f\"  Processed {i + 1:4d}/{num_datasets} datasets\")\n",
    "        \n",
    "end_time = time.time()\n",
    "loop_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nFor-loop approach took: {loop_time:.4f} seconds\")\n",
    "print(f\"Average time per dataset: {loop_time/num_datasets*1000:.2f} ms\")\n",
    "\n",
    "# Shape of result is a list of arrays, we can stack them for comparison\n",
    "posterior_means_loop = np.array(posterior_means_loop)\n",
    "print(\"Shape of stacked results:\", posterior_means_loop.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Fast Way: `lsbi` Broadcasting\n",
    "\n",
    "`lsbi` is vectorized. We can pass the entire batch of datasets `(5000, 20)` directly to the `posterior` method. `lsbi` will automatically broadcast the calculation over the first dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fast broadcasting approach\n",
    "print(\"Computing posteriors using broadcasting...\")\n",
    "start_time = time.time()\n",
    "# Pass the entire batch of data at once\n",
    "batch_posterior = model.posterior(all_y_data_noisy)\n",
    "end_time = time.time()\n",
    "broadcast_time = end_time - start_time\n",
    "\n",
    "print(f\"Broadcasting approach took: {broadcast_time:.4f} seconds\")\n",
    "print(f\"Average time per dataset: {broadcast_time/num_datasets*1000:.2f} ms\")\n",
    "\n",
    "# Check the output shape\n",
    "print(\"\\nShape of batch_posterior:\", batch_posterior.shape)\n",
    "print(\"Shape of batch_posterior.mean:\", batch_posterior.mean.shape)\n",
    "\n",
    "# Verify the results are identical\n",
    "np.testing.assert_allclose(posterior_means_loop, batch_posterior.mean, rtol=1e-10)\n",
    "print(\"\\nâœ“ Results are numerically identical!\")\n",
    "\n",
    "# Show the speedup\n",
    "speedup = loop_time / broadcast_time\n",
    "print(f\"\\nðŸš€ Speedup: {speedup:.1f}x faster with broadcasting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The broadcasting approach is not only cleaner but dramatically more performant! This speedup comes from:\n",
    "- **Vectorized operations**: NumPy's C-optimized routines\n",
    "- **Reduced Python overhead**: Single function call instead of 5000\n",
    "- **Better memory access patterns**: More cache-friendly operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance with Diagonal Covariances\n",
    "\n",
    "`lsbi` includes special, faster routines for when the covariance matrices `Î£` and `C` are diagonal. This is common when parameters or data points are assumed to be uncorrelated.\n",
    "\n",
    "Let's create a higher-dimensional problem to see the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a high-dimensional problem\n",
    "print(\"Setting up high-dimensional problem...\")\n",
    "np.random.seed(1)\n",
    "n_params_hd = 100\n",
    "n_data_hd = 500\n",
    "n_experiments = 100  # Smaller number for timing\n",
    "\n",
    "# Create a high-dimensional problem with diagonal covariances\n",
    "M_hd = np.random.rand(n_data_hd, n_params_hd)\n",
    "mu_hd = np.zeros(n_params_hd)\n",
    "Sigma_hd_diag = np.ones(n_params_hd)  # Prior variance is 1 for all params\n",
    "C_hd_diag = np.ones(n_data_hd) * 0.1   # Data variance is 0.1 for all points\n",
    "\n",
    "# Generate test data\n",
    "D_hd_batch = np.random.rand(n_experiments, n_data_hd)\n",
    "\n",
    "print(f\"Problem size: {n_params_hd} parameters, {n_data_hd} data points, {n_experiments} experiments\")\n",
    "print(f\"Total matrix operations: {n_experiments} Ã— {n_data_hd}Ã—{n_params_hd} = {n_experiments*n_data_hd*n_params_hd:,} elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1: Using full covariance matrices\n",
    "print(\"\\n=== Full Matrix Version ===\")\n",
    "model_full = LinearModel(M=M_hd, mu=mu_hd, \n",
    "                        Sigma=np.diag(Sigma_hd_diag), \n",
    "                        C=np.diag(C_hd_diag))\n",
    "\n",
    "print(\"Computing posteriors with full matrices...\")\n",
    "t_start_full = time.time()\n",
    "post_full = model_full.posterior(D_hd_batch)\n",
    "t_end_full = time.time()\n",
    "full_time = t_end_full - t_start_full\n",
    "print(f\"Full matrix version took: {full_time:.4f} seconds\")\n",
    "\n",
    "print(f\"Memory usage - Sigma: {np.diag(Sigma_hd_diag).nbytes / 1024**2:.1f} MB\")\n",
    "print(f\"Memory usage - C: {np.diag(C_hd_diag).nbytes / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 2: Using the diagonal optimization\n",
    "print(\"\\n=== Diagonal Optimization ===\")\n",
    "model_diag = LinearModel(M=M_hd, mu=mu_hd, \n",
    "                        Sigma=Sigma_hd_diag, C=C_hd_diag,\n",
    "                        diagonal_Sigma=True, diagonal_C=True)\n",
    "\n",
    "print(\"Computing posteriors with diagonal optimization...\")\n",
    "t_start_diag = time.time()\n",
    "post_diag = model_diag.posterior(D_hd_batch)\n",
    "t_end_diag = time.time()\n",
    "diag_time = t_end_diag - t_start_diag\n",
    "print(f\"Diagonal optimization took: {diag_time:.4f} seconds\")\n",
    "\n",
    "print(f\"Memory usage - Sigma: {Sigma_hd_diag.nbytes / 1024:.1f} KB\")\n",
    "print(f\"Memory usage - C: {C_hd_diag.nbytes / 1024:.1f} KB\")\n",
    "\n",
    "# Verify results are identical\n",
    "np.testing.assert_allclose(post_full.mean, post_diag.mean, rtol=1e-10)\n",
    "np.testing.assert_allclose(post_full.cov, post_diag.cov, rtol=1e-10)\n",
    "print(\"\\nâœ“ Results for full and diagonal versions are numerically identical!\")\n",
    "\n",
    "# Show the performance and memory improvements\n",
    "speedup_diag = full_time / diag_time\n",
    "memory_reduction = (np.diag(Sigma_hd_diag).nbytes + np.diag(C_hd_diag).nbytes) / (Sigma_hd_diag.nbytes + C_hd_diag.nbytes)\n",
    "\n",
    "print(f\"\\nðŸš€ Diagonal optimization speedup: {speedup_diag:.1f}x\")\n",
    "print(f\"ðŸ’¾ Memory reduction: {memory_reduction:.1f}x less memory used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common Pitfall:** When setting `diagonal_C=True` or `diagonal_Sigma=True`, you **must** pass 1D arrays for `C` and `Sigma` containing only the diagonal elements. Passing a full 2D matrix will cause an error.\n",
    "\n",
    "**Correct:**\n",
    "```python\n",
    "# For diagonal matrix with values [1, 2, 3] on diagonal\n",
    "LinearModel(C=np.array([1, 2, 3]), diagonal_C=True)  # âœ“ Correct\n",
    "```\n",
    "\n",
    "**Incorrect:**\n",
    "```python\n",
    "LinearModel(C=np.diag([1, 2, 3]), diagonal_C=True)  # âœ— Wrong!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing the Performance Scaling\n",
    "\n",
    "Let's see how the performance benefits scale with problem size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance scaling experiment\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Running performance scaling experiment...\")\n",
    "dimensions = [10, 25, 50, 100, 200]\n",
    "times_full = []\n",
    "times_diag = []\n",
    "\n",
    "for dim in dimensions:\n",
    "    print(f\"  Testing dimension {dim}...\")\n",
    "    \n",
    "    # Setup\n",
    "    M_test = np.random.rand(dim, dim)\n",
    "    D_test = np.random.rand(10, dim)  # 10 experiments\n",
    "    \n",
    "    # Full matrices\n",
    "    model_test_full = LinearModel(M=M_test, \n",
    "                                 Sigma=np.eye(dim), \n",
    "                                 C=np.eye(dim) * 0.1)\n",
    "    \n",
    "    start = time.time()\n",
    "    _ = model_test_full.posterior(D_test)\n",
    "    times_full.append(time.time() - start)\n",
    "    \n",
    "    # Diagonal optimization\n",
    "    model_test_diag = LinearModel(M=M_test, \n",
    "                                 Sigma=np.ones(dim), \n",
    "                                 C=np.ones(dim) * 0.1,\n",
    "                                 diagonal_Sigma=True, \n",
    "                                 diagonal_C=True)\n",
    "    \n",
    "    start = time.time()\n",
    "    _ = model_test_diag.posterior(D_test)\n",
    "    times_diag.append(time.time() - start)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.loglog(dimensions, times_full, 'b-o', label='Full Matrices', linewidth=2, markersize=8)\n",
    "plt.loglog(dimensions, times_diag, 'r-s', label='Diagonal Optimization', linewidth=2, markersize=8)\n",
    "plt.xlabel('Problem Dimension')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title('Performance Scaling: Full vs Diagonal Matrices')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add speedup annotations\n",
    "for i, dim in enumerate(dimensions):\n",
    "    speedup = times_full[i] / times_diag[i]\n",
    "    plt.annotate(f'{speedup:.1f}x', \n",
    "                xy=(dim, times_diag[i]), \n",
    "                xytext=(dim, times_diag[i] * 0.5),\n",
    "                ha='center', fontsize=10,\n",
    "                arrowprops=dict(arrowstyle='->', color='green', lw=1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMaximum speedup observed: {max(np.array(times_full)/np.array(times_diag)):.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Practices for High-Performance `lsbi`\n",
    "\n",
    "Based on what we've learned, here are the key performance optimization strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance best practices demonstration\n",
    "print(\"=== Performance Best Practices ===\")\n",
    "print()\n",
    "print(\"1. ðŸ“Š USE BROADCASTING:\")\n",
    "print(\"   âœ“ Pass batched data: shape (N, d) for N datasets\")\n",
    "print(\"   âœ— Avoid: for loops over individual datasets\")\n",
    "print()\n",
    "print(\"2. ðŸ”¢ USE DIAGONAL OPTIMIZATIONS:\")\n",
    "print(\"   âœ“ When parameters are independent: diagonal_Sigma=True\")\n",
    "print(\"   âœ“ When data points are independent: diagonal_C=True\")\n",
    "print(\"   âœ“ When model is diagonal: diagonal_M=True\")\n",
    "print()\n",
    "print(\"3. ðŸ’¾ MEMORY EFFICIENCY:\")\n",
    "print(\"   âœ“ Use 1D arrays for diagonal matrices\")\n",
    "print(\"   âœ“ Consider float32 for large problems if precision allows\")\n",
    "print()\n",
    "print(\"4. ðŸŽ¯ WHEN TO USE EACH APPROACH:\")\n",
    "print(\"   â€¢ Small problems (< 100 params): Either approach is fine\")\n",
    "print(\"   â€¢ Medium problems (100-1000 params): Diagonal if applicable\")\n",
    "print(\"   â€¢ Large problems (> 1000 params): Diagonal essential\")\n",
    "print(\"   â€¢ Many datasets: Always use broadcasting\")\n",
    "\n",
    "# Quick benchmark summary\n",
    "print(\"\\n=== Benchmark Summary ===\")\n",
    "print(f\"Broadcasting speedup: {speedup:.1f}x (5000 datasets)\")\n",
    "print(f\"Diagonal speedup: {max(np.array(times_full)/np.array(times_diag)):.1f}x (200D problem)\")\n",
    "print(f\"Combined potential: {speedup * max(np.array(times_full)/np.array(times_diag)):.0f}x+ speedup possible!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we explored `lsbi`'s powerful performance features:\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Broadcasting is Essential**: Pass all your datasets at once rather than looping. This can provide 50-100x+ speedups with no change in results.\n",
    "\n",
    "2. **Diagonal Optimizations Matter**: When your covariance matrices are diagonal (uncorrelated parameters/data), use the `diagonal_*=True` flags for significant speedups and memory savings.\n",
    "\n",
    "3. **Scale Matters**: Performance benefits become more pronounced with larger problems. For high-dimensional problems, diagonal optimizations can mean the difference between feasible and infeasible computations.\n",
    "\n",
    "4. **Memory Efficiency**: Diagonal optimizations reduce memory usage by factors of 100+ for large problems, enabling analysis that wouldn't fit in memory otherwise.\n",
    "\n",
    "### When to Use What:\n",
    "\n",
    "- **Always use broadcasting** when analyzing multiple datasets\n",
    "- **Use diagonal optimizations** when your problem structure allows it\n",
    "- **Combine both** for maximum performance on large-scale problems\n",
    "\n",
    "Next, try Tutorial 3 to learn about handling complex, multi-modal problems with `MixtureModel`!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}