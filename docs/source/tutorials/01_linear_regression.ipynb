{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Fitting a Straight Line with `lsbi`\n",
    "\n",
    "In this tutorial, we'll walk through the most fundamental use case for `lsbi`: fitting a straight line to noisy data. This is a classic linear regression problem, and it provides a perfect introduction to the core concepts of the library.\n",
    "\n",
    "We will cover:\n",
    "1. Setting up a synthetic dataset with a known \"ground truth\".\n",
    "2. Translating the model `y = mx + c` into the `lsbi` matrix formulation.\n",
    "3. Defining priors on our parameters (`m` and `c`).\n",
    "4. Performing inference to compute the posterior distribution.\n",
    "5. Visualizing the results to understand the information gained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Generation\n",
    "\n",
    "First, let's import the necessary libraries and create some mock data. We'll define a true slope `m` and intercept `c`, generate some data points, and add Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Data Generation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lsbi import LinearModel\n",
    "\n",
    "# Use a fixed seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Define the true model parameters\n",
    "theta_true = np.array([2.5, -1.0])  # [slope, intercept]\n",
    "n_params = len(theta_true)\n",
    "\n",
    "# 2. Generate x-values and the true y-values\n",
    "x_data = np.linspace(-2, 2, 20)\n",
    "y_data_true = theta_true[0] * x_data + theta_true[1]\n",
    "\n",
    "# 3. Add Gaussian noise to create the observed data\n",
    "data_noise_std = 0.5\n",
    "y_data_noisy = y_data_true + np.random.normal(0, data_noise_std, size=x_data.shape)\n",
    "n_data = len(x_data)\n",
    "\n",
    "# 4. Plot the data to see what we're working with\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x_data, y_data_true, 'k-', label='True Line')\n",
    "plt.errorbar(x_data, y_data_noisy, yerr=data_noise_std, fmt='o', capsize=3, label='Noisy Data')\n",
    "plt.title(\"Synthetic Dataset\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Formulating the `lsbi` Model\n",
    "\n",
    "Now comes the crucial step: translating our problem into the language `lsbi` understands. The library is based on the linear model:\n",
    "$$ D = M\\theta + m $$\n",
    "\n",
    "- `D`: The data vector. For us, this is `y_data_noisy`.\n",
    "- `θ`: The parameter vector. For us, this is `[slope, intercept]`.\n",
    "- `M`: The model matrix that maps parameters to data.\n",
    "- `m`: An optional data-space offset (we'll leave it as zero).\n",
    "\n",
    "We also need to define the prior `P(θ)` and the data covariance `C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the lsbi model components\n",
    "\n",
    "# Data vector D is our noisy y-values\n",
    "D = y_data_noisy\n",
    "\n",
    "# The model matrix M transforms [slope, intercept] into y-values.\n",
    "# y = slope * x + intercept * 1\n",
    "# This can be written in matrix form:\n",
    "# [y_1]   [x_1, 1]   [slope    ]\n",
    "# [y_2] = [x_2, 1] @ [intercept]\n",
    "# [y_3]   [x_3, 1]\n",
    "# ...     ...\n",
    "M = np.vstack([x_data, np.ones_like(x_data)]).T\n",
    "\n",
    "# Define the prior P(θ) = N(μ, Σ)\n",
    "# We'll use a broad, uninformative prior centered at zero.\n",
    "mu = np.zeros(n_params)\n",
    "# A diagonal covariance means we assume slope and intercept are a priori independent.\n",
    "# The large variance (10) means we are very uncertain.\n",
    "Sigma = np.eye(n_params) * 10 \n",
    "\n",
    "# Define the data covariance C\n",
    "# Our noise was independent and identically distributed, so C is a diagonal matrix.\n",
    "# The diagonal entries are the variance of the noise (std^2).\n",
    "C = np.eye(n_data) * data_noise_std**2\n",
    "\n",
    "print(f\"Shape of D: {D.shape}\")\n",
    "print(f\"Shape of M: {M.shape}\")\n",
    "print(f\"Shape of mu: {mu.shape}\")\n",
    "print(f\"Shape of Sigma: {Sigma.shape}\")\n",
    "print(f\"Shape of C: {C.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common Pitfall:** The most common error is constructing the `M` matrix. Always double-check that `M @ theta` produces a vector with the same shape as your data `D`. In our case, `M.shape` is `(20, 2)` and `theta.shape` is `(2,)`, so `M @ theta` gives a vector of shape `(20,)`, which matches `D`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performing Inference\n",
    "\n",
    "With all the components defined, performing inference is a single line of code. We instantiate `LinearModel` and then call the `.posterior()` method with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the inference\n",
    "# Instantiate the model\n",
    "model = LinearModel(M=M, mu=mu, Sigma=Sigma, C=C)\n",
    "\n",
    "# Compute the posterior distribution given the data D\n",
    "posterior = model.posterior(D)\n",
    "\n",
    "print(\"True parameters:\\t\", theta_true)\n",
    "print(\"Posterior mean:\\t\\t\", np.round(posterior.mean, 2))\n",
    "print(\"Posterior covariance:\\n\", np.round(posterior.cov, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior mean is very close to our true parameters! The posterior covariance matrix tells us the uncertainty in our estimate and the correlation between the slope and intercept parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyzing and Visualizing the Results\n",
    "\n",
    "Numbers are good, but plots are better. Let's visualize our result by drawing samples from the posterior distribution and plotting the corresponding lines. This shows our uncertainty in the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "# Plot the original data\n",
    "plt.errorbar(x_data, y_data_noisy, yerr=data_noise_std, fmt='o', capsize=3, label='Noisy Data', zorder=0)\n",
    "# Plot the true line\n",
    "plt.plot(x_data, y_data_true, 'k-', lw=3, label='True Line', zorder=2)\n",
    "\n",
    "# Draw 200 samples from the posterior\n",
    "for i in range(200):\n",
    "    theta_sample = posterior.rvs()\n",
    "    plt.plot(x_data, M @ theta_sample, 'r-', alpha=0.1, zorder=1)\n",
    "\n",
    "# Plot the posterior mean line\n",
    "plt.plot(x_data, M @ posterior.mean, 'r--', lw=3, label='Posterior Mean Fit', zorder=3)\n",
    "\n",
    "plt.title(\"Posterior Fit to Data\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "# Add a dummy artist for the posterior samples legend entry\n",
    "from matplotlib.lines import Line2D\n",
    "dummy_line = Line2D([0], [0], color='red', lw=1, alpha=0.5)\n",
    "plt.legend([plt.gca().lines[0], plt.gca().lines[1], dummy_line, plt.gca().lines[-1]], \n",
    "           ['Noisy Data', 'True Line', 'Posterior Samples', 'Posterior Mean Fit'])\n",
    "plt.grid(True, linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The red cloud of lines represents our posterior knowledge. It is tightly constrained where we have data and fans out where we don't, which is exactly what we expect. Our posterior mean (dashed red line) is an excellent match for the true line (solid black line)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding the Information Gain\n",
    "\n",
    "Let's compare our prior beliefs with our posterior beliefs to understand what we learned from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare prior and posterior\n",
    "prior = model.prior()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot parameter distributions\n",
    "theta_range = np.linspace(-5, 5, 100)\n",
    "\n",
    "# Slope (parameter 0)\n",
    "ax1.plot(theta_range, prior.marginalise([1]).pdf(theta_range[:, None]).flatten(), \n",
    "         'b-', label='Prior (Slope)', lw=2)\n",
    "ax1.plot(theta_range, posterior.marginalise([1]).pdf(theta_range[:, None]).flatten(), \n",
    "         'r-', label='Posterior (Slope)', lw=2)\n",
    "ax1.axvline(theta_true[0], color='k', linestyle='--', label='True Value')\n",
    "ax1.set_xlabel('Slope')\n",
    "ax1.set_ylabel('Probability Density')\n",
    "ax1.set_title('Slope Parameter')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Intercept (parameter 1)\n",
    "ax2.plot(theta_range, prior.marginalise([0]).pdf(theta_range[:, None]).flatten(), \n",
    "         'b-', label='Prior (Intercept)', lw=2)\n",
    "ax2.plot(theta_range, posterior.marginalise([0]).pdf(theta_range[:, None]).flatten(), \n",
    "         'r-', label='Posterior (Intercept)', lw=2)\n",
    "ax2.axvline(theta_true[1], color='k', linestyle='--', label='True Value')\n",
    "ax2.set_xlabel('Intercept')\n",
    "ax2.set_ylabel('Probability Density')\n",
    "ax2.set_title('Intercept Parameter')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate the information gain\n",
    "information_gain = model.dkl(D)\n",
    "print(f\"\\nInformation gained from data: {information_gain:.2f} nats\")\n",
    "print(f\"Information gained from data: {information_gain / np.log(2):.2f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we covered the essential workflow of `lsbi`:\n",
    "\n",
    "1. **Problem Setup**: We started with a simple linear model `y = mx + c`\n",
    "2. **Matrix Formulation**: We translated this into the `lsbi` framework using the model matrix `M`\n",
    "3. **Prior Definition**: We specified our prior beliefs about the parameters\n",
    "4. **Inference**: We used `model.posterior(D)` to compute the updated beliefs\n",
    "5. **Analysis**: We visualized and interpreted the results\n",
    "\n",
    "Key takeaways:\n",
    "- The model matrix `M` is crucial - it defines how parameters map to data\n",
    "- `lsbi` provides exact analytical solutions (no sampling required)\n",
    "- The posterior uncertainty naturally reflects the information content of your data\n",
    "- Visualization helps interpret the meaning of the mathematical results\n",
    "\n",
    "Next, try Tutorial 2 to learn about `lsbi`'s powerful broadcasting capabilities for analyzing many datasets simultaneously!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}